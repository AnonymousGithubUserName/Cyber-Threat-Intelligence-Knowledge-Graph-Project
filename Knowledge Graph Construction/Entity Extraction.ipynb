{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Packages and Functions\n",
    "import glob\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy,stanza,sys,re,pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import fuzz\n",
    "from gensim.models import Word2Vec\n",
    "from iocparser import IOCParser\n",
    "import networkx as nx\n",
    "from nltk.corpus import words\n",
    "from scipy import spatial\n",
    "from simpletransformers.ner import NERModel\n",
    "from stanza.server import CoreNLPClient\n",
    "import stanza.protobuf.CoreNLP_pb2 as CoreNLP_pb2\n",
    "from tqdm.contrib import tzip\n",
    "\n",
    "import stanza\n",
    "corenlp_dir = '/home/user/corenlp'\n",
    "from stanza.server import CoreNLPClient\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] ='/home/user/corenlp'\n",
    "word_set = set(words.words())\n",
    "sys.modules['CoreNLP_pb2'] = CoreNLP_pb2\n",
    "\n",
    "w2v = pickle.load(open('pkl/w2v_ref_blog_lemma.pkl', 'rb'))\n",
    "\n",
    "mitre_groups = pickle.load(open('pkl/mitre_groups.pkl', 'rb'))\n",
    "mitre_malwares = pickle.load(open('pkl/mitre_malwares.pkl', 'rb'))\n",
    "programm_language = pickle.load(open('pkl/programming_language.pkl', 'rb'))\n",
    "\n",
    "mitre_groups = set(list(map(str.lower, mitre_groups)))\n",
    "mitre_malwares = set(list(map(str.lower, mitre_malwares)))\n",
    "\n",
    "malware = pickle.load(open('pkl/malware.pkl', 'rb'))\n",
    "software = pickle.load(open('pkl/software.pkl', 'rb'))\n",
    "actor = pickle.load(open('pkl/actor.pkl', 'rb'))\n",
    "\n",
    "mal_suffix = ['rat', 'downloader', 'trojan', 'ransomware', 'malware', 'virus', 'worm', 'backdoor']\n",
    "for malname in list(malware):\n",
    "    if ' ' in malname:\n",
    "        for _ in mal_suffix:\n",
    "            if _ in malname.split(' '):\n",
    "                malware.remove(malname)\n",
    "                malware.add(''.join(filter(lambda x: x not in mal_suffix, malname.split(' '))))\n",
    "\n",
    "for malname in list(malware):\n",
    "    if malname in word_set:\n",
    "        malware.remove(malname)\n",
    "\n",
    "for softname in list(software):\n",
    "    if softname in word_set:\n",
    "        software.remove(softname)\n",
    "\n",
    "for actname in list(actor):\n",
    "    if actname in word_set:\n",
    "        actor.remove(actname)\n",
    "\n",
    "#%%\n",
    "import stanza\n",
    "corenlp_dir = '/home/user/corenlp'\n",
    "from stanza.server import CoreNLPClient\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] ='/home/user/corenlp'\n",
    "word_set = set(words.words())\n",
    "sys.modules['CoreNLP_pb2'] = CoreNLP_pb2\n",
    "\n",
    "\n",
    "w2v = pickle.load(open('pkl/w2v_ref_blog_lemma.pkl', 'rb'))\n",
    "mitre_groups = pickle.load(open('pkl/mitre_groups.pkl', 'rb'))\n",
    "mitre_malwares = pickle.load(open('pkl/mitre_malwares.pkl', 'rb'))\n",
    "programm_language = pickle.load(open('pkl/programming_language.pkl', 'rb'))\n",
    "\n",
    "mitre_groups = set(list(map(str.lower, mitre_groups)))\n",
    "mitre_malwares = set(list(map(str.lower, mitre_malwares)))\n",
    "\n",
    "malware = pickle.load(open('pkl/malware.pkl', 'rb'))\n",
    "software = pickle.load(open('pkl/software.pkl', 'rb'))\n",
    "actor = pickle.load(open('pkl/actor.pkl', 'rb'))\n",
    "\n",
    "mal_suffix = ['rat', 'downloader', 'trojan', 'ransomware', 'malware', 'virus', 'worm', 'backdoor']\n",
    "for malname in list(malware):\n",
    "    if ' ' in malname:\n",
    "        for _ in mal_suffix:\n",
    "            if _ in malname.split(' '):\n",
    "                malware.remove(malname)\n",
    "                malware.add(''.join(filter(lambda x: x not in mal_suffix, malname.split(' '))))\n",
    "\n",
    "for malname in list(malware):\n",
    "    if malname in word_set:\n",
    "        malware.remove(malname)\n",
    "\n",
    "for softname in list(software):\n",
    "    if softname in word_set:\n",
    "        software.remove(softname)\n",
    "\n",
    "for actname in list(actor):\n",
    "    if actname in word_set:\n",
    "        actor.remove(actname)\n",
    "\n",
    "class Triple():\n",
    "    def __init__(self, sub, rel, obj, sub_ent='', rel_ent='', obj_ent='', sub_ent_conf=0, obj_ent_conf=0, conf=0):\n",
    "        self.sub = sub.lower()\n",
    "        self.rel = rel.lower()\n",
    "        self.obj = obj.lower()\n",
    "        self.sub_ent = sub_ent\n",
    "        self.rel_ent = rel_ent\n",
    "        self.obj_ent = obj_ent\n",
    "        self.sub_ent_conf = sub_ent_conf\n",
    "        self.obj_ent_conf = obj_ent_conf\n",
    "        self.confidence = conf\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.simple_sub + self.simple_rel + self.simple_obj)\n",
    "\n",
    "    def set_raw_article_doc(self, doc):\n",
    "        self.article_doc = doc\n",
    "\n",
    "    def set_raw_sent(self, sent):\n",
    "        self.raw_sent = sent\n",
    "\n",
    "    def set_raw_triple(self, triple_obj):\n",
    "        self.triple_obj = triple_obj\n",
    "\n",
    "    def set_sent_ind(self, sent_ind):\n",
    "        self.sent_ind = sent_ind\n",
    "\n",
    "    def set_article_ref(self, article):\n",
    "        self.article_id = article\n",
    "\n",
    "    def set_tactic(self, tactic):\n",
    "        self.tactic = tactic\n",
    "\n",
    "    def set_tactic_conf(self, tactic_conf):\n",
    "        self.tactic_conf = tactic_conf\n",
    "\n",
    "    def set_behave_conf(self, behave_conf):\n",
    "        self.behave_conf = behave_conf\n",
    "\n",
    "    def set_relative_sent_ind(self, relative_sent_order):\n",
    "        self.relative_sent_order = relative_sent_order\n",
    "\n",
    "    def set_count(self, count):\n",
    "        self.count = count\n",
    "\n",
    "    def add_cluster(self, cluster):\n",
    "        self.from_cluster = set()\n",
    "        self.from_cluster.add(cluster)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        sub_eq = False\n",
    "        if self.sub in other.sub or other.sub in self.sub:\n",
    "            sub_eq = True\n",
    "        elif fuzz.ratio(self.sub, other.sub) > 90:\n",
    "            sub_eq = True\n",
    "        else:\n",
    "            self_sub_tokens = self.sub.split(' ')\n",
    "            other_sub_tokens = other.sub.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_sub_tokens)) and all(map(lambda x: x in w2v.wv, other_sub_tokens)):\n",
    "                self_sub_vec = np.mean([w2v.wv[x] for x in self_sub_tokens], axis=0)\n",
    "                other_sub_vec = np.mean([w2v.wv[x] for x in other_sub_tokens], axis=0)\n",
    "                sub_eq = spatial.distance.cosine(self_sub_vec, other_sub_vec) < 0.3\n",
    "\n",
    "        rel_eq = False\n",
    "        if self.rel in other.rel or other.rel in self.rel:\n",
    "            rel_eq = True\n",
    "        elif self.rel == other.rel:\n",
    "            rel_eq = True\n",
    "        else:\n",
    "            self_rel_tokens = self.rel.split(' ')\n",
    "            other_rel_tokens = other.rel.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_rel_tokens)) and all(map(lambda x: x in w2v.wv, other_rel_tokens)):\n",
    "                self_rel_vec = np.mean([w2v.wv[x] for x in self_rel_tokens], axis=0)\n",
    "                other_rel_vec = np.mean([w2v.wv[x] for x in other_rel_tokens], axis=0)\n",
    "                rel_eq = spatial.distance.cosine(self_rel_vec, other_rel_vec) < 0.3\n",
    "\n",
    "        obj_eq = False\n",
    "        if self.obj in other.obj or other.obj in self.obj:\n",
    "            obj_eq = True\n",
    "        elif fuzz.ratio(self.obj, other.obj) > 90:\n",
    "            obj_eq = True\n",
    "        else:\n",
    "            self_obj_tokens = self.obj.split(' ')\n",
    "            other_obj_tokens = other.obj.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_obj_tokens)) and all(map(lambda x: x in w2v.wv, other_obj_tokens)):\n",
    "                self_obj_vec = np.mean([w2v.wv[x] for x in self_obj_tokens], axis=0)\n",
    "                other_obj_vec = np.mean([w2v.wv[x] for x in other_obj_tokens], axis=0)\n",
    "                obj_eq = spatial.distance.cosine(self_obj_vec, other_obj_vec) < 0.3\n",
    "\n",
    "        return sub_eq and rel_eq and obj_eq and self.sub_ent == other.sub_ent and self.obj_ent == other.obj_ent and self.rel_ent == other.rel_ent\n",
    "\n",
    "    def print(self):\n",
    "        use_less='1'\n",
    "\n",
    "def bert_ner(ann):\n",
    "    tokenized = [[x.value for x in sent.token] for sent in ann.sentence]\n",
    "    predictions, raw_outputs = model.predict(tokenized, split_on_space=False)\n",
    "    return predictions, raw_outputs\n",
    "\n",
    "def simple_match(candidate, ioc_dic):\n",
    "    candidate = candidate.lower().strip()\n",
    "    if (candidate.startswith(\"cve\")):\n",
    "        return \"CVE\"\n",
    "    for _ in mitre_groups:\n",
    "        if candidate.find(_) != -1:\n",
    "            return \"Known_Actor\"\n",
    "    for _ in mitre_malwares:\n",
    "        if candidate.find(_) != -1:\n",
    "            return \"Known_Malware\"\n",
    "    for _ in programm_language:\n",
    "        if _ == candidate:\n",
    "            return \"Programming_Language\"\n",
    "    for _ in ioc_dic:\n",
    "        if _ in candidate:\n",
    "            textobj = IOCParser(ioc_dic[_]);\n",
    "            results = textobj.parse();\n",
    "            if len(results) > 0:\n",
    "                return results[0].kind\n",
    "    return ''\n",
    "\n",
    "\n",
    "def process_triple_tokens(tokens, ann, predictions, raws, ioc_dic):\n",
    "    redundant = ['PRP$', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'DT']\n",
    "    inflex = ['VBD', 'VBG', 'VBP', 'VBZ', 'NNS', 'NNPS']\n",
    "    sub_tokens = tokens\n",
    "    simplified_sub_tokens = []\n",
    "    sub_entity = ''\n",
    "    sub_ner_conf = 0\n",
    "    for sub_token in sub_tokens:\n",
    "        token_obj = ann.sentence[sub_token.sentenceIndex].token[sub_token.tokenIndex]\n",
    "        if token_obj.pos.startswith('NN'):\n",
    "            ner_tag = 'O'\n",
    "            try:\n",
    "                ner_tag = list(predictions[sub_token.sentenceIndex][sub_token.tokenIndex].values())[0]\n",
    "                sub_ner_conf = max(\n",
    "                    scipy.special.softmax(list(raws[sub_token.sentenceIndex][sub_token.tokenIndex].values())[0][0]))\n",
    "            except:\n",
    "                pass\n",
    "            if ner_tag.startswith('B'):\n",
    "                sub_entity = ner_tag.split('-')[1]\n",
    "            if token_obj.pos in inflex:\n",
    "                simplified_sub_tokens.append(token_obj.lemma)\n",
    "            else:\n",
    "                simplified_sub_tokens.append(token_obj.word)\n",
    "        elif token_obj.pos not in redundant:\n",
    "            simplified_sub_tokens.append(token_obj.word)\n",
    "    if len(simplified_sub_tokens) == 0:\n",
    "        return None, None, None\n",
    "    else:\n",
    "        sub = \" \".join(simplified_sub_tokens)\n",
    "        simple_ent = simple_match(sub, ioc_dic)\n",
    "        if simple_ent != '':\n",
    "            sub_entity = simple_ent\n",
    "            sub_ner_conf = 1\n",
    "    return sub, sub_entity, sub_ner_conf\n",
    "\n",
    "\n",
    "def extract_triples_with_ner(ann, predictions, raws, ioc_dic, doc_id=None):\n",
    "    inflex = ['VBD', 'VBG', 'VBP', 'VBZ', 'NNS', 'NNPS']\n",
    "    extracted = list()\n",
    "    sub_counter = Counter()\n",
    "    obj_counter = Counter()\n",
    "    relation_counter = Counter()\n",
    "    for sent_ind, sent in enumerate(ann.sentence):\n",
    "        triples = sent.openieTriple\n",
    "        for triple in triples:\n",
    "            #print(triple)\n",
    "            sub_tokens = triple.subjectTokens\n",
    "            sub, sub_entity, sub_ner_conf = process_triple_tokens(sub_tokens, ann, predictions, raws, ioc_dic)\n",
    "            if not sub:\n",
    "                continue\n",
    "\n",
    "            obj_tokens = triple.objectTokens\n",
    "            obj, obj_entity, obj_ner_conf = process_triple_tokens(obj_tokens, ann, predictions, raws, ioc_dic)\n",
    "            if not obj:\n",
    "                continue\n",
    "\n",
    "            rel_tokens = triple.relationTokens\n",
    "            simplified_rel_tokens = []\n",
    "            for rel_token in rel_tokens:\n",
    "                token_obj = ann.sentence[rel_token.sentenceIndex].token[rel_token.tokenIndex]\n",
    "                if token_obj.pos != 'RB':\n",
    "                    if token_obj.pos in inflex:\n",
    "                        simplified_rel_tokens.append(token_obj.lemma)\n",
    "                    else:\n",
    "                        simplified_rel_tokens.append(token_obj.word)\n",
    "            if len(simplified_rel_tokens) == 0:\n",
    "                #print(\"no rel\")\n",
    "                continue\n",
    "            else:\n",
    "                rel = \" \".join(simplified_rel_tokens)\n",
    "\n",
    "            for k, v in ioc_dic.items():\n",
    "                if k in sub:\n",
    "                    sub = sub.replace(k, v)\n",
    "                if k in obj:\n",
    "                    obj = obj.replace(k, v)\n",
    "                if k in rel:\n",
    "                    rel = rel.replace(k, v)\n",
    "\n",
    "            tri = Triple(sub, rel, obj, sub_ent=sub_entity, obj_ent=obj_entity, sub_ent_conf=sub_ner_conf,\n",
    "                         obj_ent_conf=obj_ner_conf, conf=triple.confidence)\n",
    "            tri.set_sent_ind(sent_ind)\n",
    "            tri.set_relative_sent_ind(sent_ind / len(ann.sentence))\n",
    "            tri.set_article_ref(doc_id)\n",
    "            tri.set_raw_triple(triple)\n",
    "            extracted.append(tri)\n",
    "            sub_counter.update([sub])\n",
    "            obj_counter.update([obj])\n",
    "            relation_counter.update([rel])\n",
    "    return extracted, (sub_counter, obj_counter), relation_counter\n",
    "\n",
    "\n",
    "def change_ioc(text):\n",
    "    count = 0\n",
    "    dic = dict()\n",
    "    textobj = IOCParser(text);\n",
    "    results = textobj.parse();\n",
    "    for res in results:\n",
    "        sig = res.kind + str(count)\n",
    "        dic[sig] = res.value\n",
    "        count += 1\n",
    "        text = text.replace(res.value, sig)\n",
    "    return text, dic\n",
    "\n",
    "tactic_dic = {\n",
    "    0: 'Initial Access', \n",
    "    1: 'Execution', \n",
    "    2: 'Defense Evasion', \n",
    "    3: 'Command and Control', \n",
    "    4: 'Privilege Escalation', \n",
    "    5: 'Persistence', \n",
    "    6: 'Lateral Movement', \n",
    "    7: 'DataLeak', \n",
    "    8: 'Exfiltration', \n",
    "    9: 'Impact'\n",
    "}\n",
    "\n",
    "valid_ner = [\n",
    "         'Known_Malware',\n",
    "         'PRO',\n",
    "         'PERSON',\n",
    "         'filename',\n",
    "         'uri',\n",
    "         'md5',\n",
    "         'ACTOR',\n",
    "         'Known_Actor',\n",
    "         'sha256',\n",
    "         'URL',\n",
    "         'sha1',\n",
    "         'CVE',\n",
    "]\n",
    "\n",
    "def dedupe(article_extracts):\n",
    "    seen_sent = dict()\n",
    "    unique = []\n",
    "    for triple in article_extracts:\n",
    "        sent_ind = triple.sent_ind\n",
    "        if sent_ind not in seen_sent:\n",
    "            seen_sent[sent_ind] = list()\n",
    "        \n",
    "        dup = False\n",
    "        for seen_triple in seen_sent[sent_ind]:\n",
    "            if seen_triple == triple:\n",
    "                if len(seen_triple.sub) >= len(triple.sub) and len(seen_triple.obj) >= len(triple.obj):\n",
    "                    seen_sent[sent_ind].remove(seen_triple)\n",
    "                    dup = False\n",
    "                else:\n",
    "                    #seen_triple.set_count(seen_triple.count+1)\n",
    "                    dup = True\n",
    "                break\n",
    "                    \n",
    "        if not dup:\n",
    "            seen_sent[sent_ind].append(triple)\n",
    "    \n",
    "    for k, v in seen_sent.items():\n",
    "        unique += v\n",
    "        \n",
    "    return unique\n",
    "  \n",
    "def fusion(trip):\n",
    "    det_ner = ['filename', 'sha256', 'CVE', 'uri', 'md5', 'sha1']\n",
    "    if trip.sub_ent not in det_ner:\n",
    "        for single in malware:\n",
    "            #if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub = single\n",
    "                trip.sub_ent = 'Known_Malware'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub = single\n",
    "                trip.sub_ent = 'Known_Malware'\n",
    "                trip.sub_ent_conf = 1\n",
    "                \n",
    "        for single in software:\n",
    "           # if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub_ent = 'PRO'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub_ent = 'PRO'\n",
    "                trip.sub_ent_conf = 1\n",
    "                \n",
    "        for single in actor:\n",
    "            #if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub_ent = 'Known_Actor'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub_ent = 'Known_Actor'\n",
    "                trip.sub_ent_conf = 1\n",
    "    \n",
    "    if trip.obj_ent not in det_ner:\n",
    "        for single in malware:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj = single\n",
    "                trip.obj_ent = 'Known_Malware'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj = single\n",
    "                trip.obj_ent = 'Known_Malware'\n",
    "                trip.obj_ent_conf = 1\n",
    "                \n",
    "        for single in software:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj_ent = 'PRO'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj_ent = 'PRO'\n",
    "                trip.obj_ent_conf = 1\n",
    "                \n",
    "        for single in actor:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj_ent = 'Known_Actor'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj_ent = 'Known_Actor'\n",
    "                trip.obj_ent_conf = 1\n",
    "\n",
    "def filter_ner(triples):\n",
    "    filtered = []\n",
    "    for triple in triples:\n",
    "        if triple.sub_ent != '' and triple.sub_ent in valid_ner and triple.obj_ent != '' and triple.obj_ent in valid_ner:\n",
    "            filtered.append(triple)\n",
    "            \n",
    "    return filtered\n",
    "\n",
    "def filter_conf(triples, behav_thresh=0.5):\n",
    "    filtered = []\n",
    "    for triple in triples:        \n",
    "        if triple.confidence < 0.8:\n",
    "            continue\n",
    "            \n",
    "        if triple.behave_conf < behav_thresh:\n",
    "            continue\n",
    "        \n",
    "        filtered.append(triple)\n",
    "    return filtered\n",
    "\n",
    "import numpy as np\n",
    "import glob,pickle\n",
    "import sys\n",
    "from simpletransformers.classification import ClassificationModel,    ClassificationArgs,    MultiLabelClassificationModel # type: ignore\n",
    "\n",
    "from simpletransformers.ner import NERModel\n",
    "model = NERModel(\"roberta\", \"nermodel\",use_cuda=True)\n",
    "be_model=MultiLabelClassificationModel(\"roberta\", 'behavior model',use_cuda=True)\n",
    "ta_model=MultiLabelClassificationModel(\"roberta\", 'tactic model',use_cuda=True)\n",
    "\n",
    "def find_zeros(lst):\n",
    "    zeros_count = 0\n",
    "    zeros_indices = []\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] == 0:\n",
    "            zeros_count += 1\n",
    "        else:\n",
    "            if zeros_count > 0:\n",
    "                zeros_indices.append(i - zeros_count)\n",
    "            zeros_count = 0\n",
    "    if zeros_count > 0:\n",
    "        zeros_indices.append(len(lst) - zeros_count)\n",
    "    return zeros_indices\n",
    "\n",
    "def nlp_process(dict_text_ann):\n",
    "  anns = list(dict_text_ann.values())\n",
    "  keys = list(dict_text_ann.keys())\n",
    "  texts=[]\n",
    "  tokenized_s=[]\n",
    "  anns_noerror=[]\n",
    "  for ann in anns:\n",
    "    if ann != 'error':\n",
    "      anns_noerror.append(ann)\n",
    "      tokenized_s.append([[x.value for x in sent.token] for sent in ann.sentence])\n",
    "      texts.append(ann.text)\n",
    "\n",
    "  tokenized_s=[x[0] for x in tokenized_s]\n",
    "  prediction, raw_output = model.predict(tokenized_s, split_on_space=False)\n",
    "\n",
    "  extracted_listS=[]\n",
    "  for i in range(len(anns_noerror)):\n",
    "    ann=anns_noerror[i]\n",
    "    ner=prediction[i]\n",
    "    raw_out=raw_output[i]\n",
    "    notext, ioc_dic = change_ioc(texts[i])\n",
    "    extracted_list, entity_count, rel_count = extract_triples_with_ner(ann, ner, raw_out, ioc_dic, doc_id = 1)\n",
    "    extracted_listS.append(extracted_list)\n",
    "\n",
    "  text_list=texts\n",
    "\n",
    "  be_predictions, be_raw_outputs = be_model.predict(text_list)\n",
    "  ta_predictions, ta_raw_outputs = ta_model.predict(text_list)\n",
    "\n",
    "  for i in range(len(extracted_listS)):\n",
    "      if len(be_predictions[i])>0:\n",
    "          for triple in extracted_listS[i]:\n",
    "              triple.raw_sent=text_list[i]\n",
    "              triple.behave_conf=be_raw_outputs[i][1]\n",
    "              triple.tactic=find_zeros(ta_predictions[i])\n",
    "              triple.sent_ind=i\n",
    "              triple.tactic_conf=ta_predictions[i]\n",
    "\n",
    "  for i in range(len(extracted_listS)):\n",
    "    extracted_listS[i]=dedupe(extracted_listS[i])\n",
    "  for i in range(len(extracted_listS)):\n",
    "    for j in range(len(extracted_listS[i])):\n",
    "      fusion(extracted_listS[i][j])\n",
    "  return extracted_listS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performs Entity Extraction on all files in the directory\n",
    "import os\n",
    "base_dir = '/home/user'\n",
    "files = glob.glob(os.path.join(base_dir, 'noerror', '*.pkl'))\n",
    "processed_files = glob.glob(os.path.join(base_dir, 'noerrorfinal', '*.pkl'))\n",
    "\n",
    "processed_files_shortname = {os.path.basename(pf).replace('processed', '') for pf in processed_files}\n",
    "\n",
    "# Use list comprehension to filter out processed files\n",
    "files = [f for f in files if os.path.basename(f) not in processed_files_shortname]\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'rb') as f:\n",
    "        try:\n",
    "            my_dict = pickle.load(f)\n",
    "            out = nlp_process(my_dict);\n",
    "            out_file = os.path.join('/home/user', 'processed' + os.path.basename(file))\n",
    "            with open(out_file, 'wb') as f_out:\n",
    "                pickle.dump(out, f_out)\n",
    "        except Exception as e:\n",
    "            with open('error_afterpipeline.txt', 'a') as f_err:\n",
    "                #print('error!')\n",
    "                f_err.write(file + '\\n')\n",
    "                f_err.write(str(e) + '\\n')\n",
    "\n",
    "with open(file, 'rb') as f:\n",
    "      my_dict=pickle.load(f)\n",
    "      out=nlp_process(my_dict)\n",
    "\n",
    "print('no output');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf0194bd040dd46a0b7c04c9ab4ea3b5ed434e6c6c0698aa64ad071849497352"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
