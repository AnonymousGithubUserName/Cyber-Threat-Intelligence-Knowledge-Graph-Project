{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package And Function\n",
    "import glob\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy,stanza,sys,re,pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import fuzz\n",
    "from gensim.models import Word2Vec\n",
    "from iocparser import IOCParser\n",
    "import networkx as nx\n",
    "from nltk.corpus import words\n",
    "from scipy import spatial\n",
    "from simpletransformers.ner import NERModel\n",
    "from stanza.server import CoreNLPClient\n",
    "import stanza.protobuf.CoreNLP_pb2 as CoreNLP_pb2\n",
    "from tqdm.contrib import tzip\n",
    "\n",
    "import stanza\n",
    "corenlp_dir = '/home/user/anaconda3/data/corenlp'\n",
    "from stanza.server import CoreNLPClient\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] ='/home/user/anaconda3/data/corenlp'\n",
    "word_set = set(words.words())\n",
    "sys.modules['CoreNLP_pb2'] = CoreNLP_pb2\n",
    "\n",
    "\n",
    "w2v = pickle.load(open('pkl/w2v_ref_blog_lemma.pkl', 'rb'))\n",
    "\n",
    "mitre_groups = pickle.load(open('pkl/mitre_groups.pkl', 'rb'))\n",
    "mitre_malwares = pickle.load(open('pkl/mitre_malwares.pkl', 'rb'))\n",
    "programm_language = pickle.load(open('pkl/programming_language.pkl', 'rb'))\n",
    "\n",
    "mitre_groups = set(list(map(str.lower, mitre_groups)))\n",
    "mitre_malwares = set(list(map(str.lower, mitre_malwares)))\n",
    "\n",
    "malware = pickle.load(open('pkl/malware.pkl', 'rb'))\n",
    "software = pickle.load(open('pkl/software.pkl', 'rb'))\n",
    "actor = pickle.load(open('pkl/actor.pkl', 'rb'))\n",
    "\n",
    "mal_suffix = ['rat', 'downloader', 'trojan', 'ransomware', 'malware', 'virus', 'worm', 'backdoor']\n",
    "for malname in list(malware):\n",
    "    if ' ' in malname:\n",
    "        for _ in mal_suffix:\n",
    "            if _ in malname.split(' '):\n",
    "                malware.remove(malname)\n",
    "                malware.add(''.join(filter(lambda x: x not in mal_suffix, malname.split(' '))))\n",
    "\n",
    "for malname in list(malware):\n",
    "    if malname in word_set:\n",
    "        malware.remove(malname)\n",
    "\n",
    "for softname in list(software):\n",
    "    if softname in word_set:\n",
    "        software.remove(softname)\n",
    "\n",
    "for actname in list(actor):\n",
    "    if actname in word_set:\n",
    "        actor.remove(actname)\n",
    "\n",
    "\n",
    "class Triple():\n",
    "    def __init__(self, sub, rel, obj, sub_ent='', rel_ent='', obj_ent='', sub_ent_conf=0, obj_ent_conf=0, conf=0):\n",
    "        self.sub = sub.lower()\n",
    "        self.rel = rel.lower()\n",
    "        self.obj = obj.lower()\n",
    "        self.sub_ent = sub_ent\n",
    "        self.rel_ent = rel_ent\n",
    "        self.obj_ent = obj_ent\n",
    "        self.sub_ent_conf = sub_ent_conf\n",
    "        self.obj_ent_conf = obj_ent_conf\n",
    "        self.confidence = conf\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.simple_sub + self.simple_rel + self.simple_obj)\n",
    "\n",
    "    def set_raw_article_doc(self, doc):\n",
    "        self.article_doc = doc\n",
    "\n",
    "    def set_raw_sent(self, sent):\n",
    "        self.raw_sent = sent\n",
    "\n",
    "    def set_raw_triple(self, triple_obj):\n",
    "        self.triple_obj = triple_obj\n",
    "\n",
    "    def set_sent_ind(self, sent_ind):\n",
    "        self.sent_ind = sent_ind\n",
    "\n",
    "    def set_article_ref(self, article):\n",
    "        self.article_id = article\n",
    "\n",
    "    def set_tactic(self, tactic):\n",
    "        self.tactic = tactic\n",
    "\n",
    "    def set_tactic_conf(self, tactic_conf):\n",
    "        self.tactic_conf = tactic_conf\n",
    "\n",
    "    def set_behave_conf(self, behave_conf):\n",
    "        self.behave_conf = behave_conf\n",
    "\n",
    "    def set_relative_sent_ind(self, relative_sent_order):\n",
    "        self.relative_sent_order = relative_sent_order\n",
    "\n",
    "    def set_count(self, count):\n",
    "        self.count = count\n",
    "\n",
    "    def add_cluster(self, cluster):\n",
    "        self.from_cluster = set()\n",
    "        self.from_cluster.add(cluster)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        sub_eq = False\n",
    "        if self.sub in other.sub or other.sub in self.sub:\n",
    "            sub_eq = True\n",
    "        elif fuzz.ratio(self.sub, other.sub) > 90:\n",
    "            sub_eq = True\n",
    "        else:\n",
    "            self_sub_tokens = self.sub.split(' ')\n",
    "            other_sub_tokens = other.sub.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_sub_tokens)) and all(map(lambda x: x in w2v.wv, other_sub_tokens)):\n",
    "                self_sub_vec = np.mean([w2v.wv[x] for x in self_sub_tokens], axis=0)\n",
    "                other_sub_vec = np.mean([w2v.wv[x] for x in other_sub_tokens], axis=0)\n",
    "                sub_eq = spatial.distance.cosine(self_sub_vec, other_sub_vec) < 0.3\n",
    "\n",
    "        rel_eq = False\n",
    "        if self.rel in other.rel or other.rel in self.rel:\n",
    "            rel_eq = True\n",
    "        elif self.rel == other.rel:\n",
    "            rel_eq = True\n",
    "        else:\n",
    "            self_rel_tokens = self.rel.split(' ')\n",
    "            other_rel_tokens = other.rel.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_rel_tokens)) and all(map(lambda x: x in w2v.wv, other_rel_tokens)):\n",
    "                self_rel_vec = np.mean([w2v.wv[x] for x in self_rel_tokens], axis=0)\n",
    "                other_rel_vec = np.mean([w2v.wv[x] for x in other_rel_tokens], axis=0)\n",
    "                rel_eq = spatial.distance.cosine(self_rel_vec, other_rel_vec) < 0.3\n",
    "\n",
    "        obj_eq = False\n",
    "        if self.obj in other.obj or other.obj in self.obj:\n",
    "            obj_eq = True\n",
    "        elif fuzz.ratio(self.obj, other.obj) > 90:\n",
    "            obj_eq = True\n",
    "        else:\n",
    "            self_obj_tokens = self.obj.split(' ')\n",
    "            other_obj_tokens = other.obj.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_obj_tokens)) and all(map(lambda x: x in w2v.wv, other_obj_tokens)):\n",
    "                self_obj_vec = np.mean([w2v.wv[x] for x in self_obj_tokens], axis=0)\n",
    "                other_obj_vec = np.mean([w2v.wv[x] for x in other_obj_tokens], axis=0)\n",
    "                obj_eq = spatial.distance.cosine(self_obj_vec, other_obj_vec) < 0.3\n",
    "\n",
    "        return sub_eq and rel_eq and obj_eq and self.sub_ent == other.sub_ent and self.obj_ent == other.obj_ent and self.rel_ent == other.rel_ent\n",
    "\n",
    "    def print(self):\n",
    "        uselessword='1'\n",
    "\n",
    "\n",
    "def bert_ner(ann):\n",
    "    tokenized = [[x.value for x in sent.token] for sent in ann.sentence]\n",
    "    predictions, raw_outputs = model.predict(tokenized, split_on_space=False)\n",
    "    return predictions, raw_outputs\n",
    "\n",
    "\n",
    "def simple_match(candidate, ioc_dic):\n",
    "    candidate = candidate.lower().strip()\n",
    "    if (candidate.startswith(\"cve\")):\n",
    "        return \"CVE\"\n",
    "    for _ in mitre_groups:\n",
    "        if candidate.find(_) != -1:\n",
    "            return \"Known_Actor\"\n",
    "    for _ in mitre_malwares:\n",
    "        if candidate.find(_) != -1:\n",
    "            return \"Known_Malware\"\n",
    "    for _ in programm_language:\n",
    "        if _ == candidate:\n",
    "            return \"Programming_Language\"\n",
    "    for _ in ioc_dic:\n",
    "        if _ in candidate:\n",
    "            textobj = IOCParser(ioc_dic[_]);\n",
    "            results = textobj.parse();\n",
    "            if len(results) > 0:\n",
    "                return results[0].kind\n",
    "    return ''\n",
    "\n",
    "\n",
    "def process_triple_tokens(tokens, ann, predictions, raws, ioc_dic):\n",
    "    redundant = ['PRP$', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'DT']\n",
    "    inflex = ['VBD', 'VBG', 'VBP', 'VBZ', 'NNS', 'NNPS']\n",
    "    sub_tokens = tokens\n",
    "    simplified_sub_tokens = []\n",
    "    sub_entity = ''\n",
    "    sub_ner_conf = 0\n",
    "    for sub_token in sub_tokens:\n",
    "        token_obj = ann.sentence[sub_token.sentenceIndex].token[sub_token.tokenIndex]\n",
    "        if token_obj.pos.startswith('NN'):\n",
    "            ner_tag = 'O'\n",
    "            try:\n",
    "                ner_tag = list(predictions[sub_token.sentenceIndex][sub_token.tokenIndex].values())[0]\n",
    "                sub_ner_conf = max(\n",
    "                    scipy.special.softmax(list(raws[sub_token.sentenceIndex][sub_token.tokenIndex].values())[0][0]))\n",
    "            except:\n",
    "                pass\n",
    "            if ner_tag.startswith('B'):\n",
    "                sub_entity = ner_tag.split('-')[1]\n",
    "            if token_obj.pos in inflex:\n",
    "                simplified_sub_tokens.append(token_obj.lemma)\n",
    "            else:\n",
    "                simplified_sub_tokens.append(token_obj.word)\n",
    "        elif token_obj.pos not in redundant:\n",
    "            simplified_sub_tokens.append(token_obj.word)\n",
    "    if len(simplified_sub_tokens) == 0:\n",
    "        return None, None, None\n",
    "    else:\n",
    "        sub = \" \".join(simplified_sub_tokens)\n",
    "        simple_ent = simple_match(sub, ioc_dic)\n",
    "        if simple_ent != '':\n",
    "            sub_entity = simple_ent\n",
    "            sub_ner_conf = 1\n",
    "    return sub, sub_entity, sub_ner_conf\n",
    "\n",
    "\n",
    "def extract_triples_with_ner(ann, predictions, raws, ioc_dic, doc_id=None):\n",
    "    inflex = ['VBD', 'VBG', 'VBP', 'VBZ', 'NNS', 'NNPS']\n",
    "    extracted = list()\n",
    "    sub_counter = Counter()\n",
    "    obj_counter = Counter()\n",
    "    relation_counter = Counter()\n",
    "    for sent_ind, sent in enumerate(ann.sentence):\n",
    "        triples = sent.openieTriple\n",
    "        for triple in triples:\n",
    "            #print(triple)\n",
    "            sub_tokens = triple.subjectTokens\n",
    "            sub, sub_entity, sub_ner_conf = process_triple_tokens(sub_tokens, ann, predictions, raws, ioc_dic)\n",
    "            if not sub:\n",
    "                continue\n",
    "\n",
    "            obj_tokens = triple.objectTokens\n",
    "            obj, obj_entity, obj_ner_conf = process_triple_tokens(obj_tokens, ann, predictions, raws, ioc_dic)\n",
    "            if not obj:\n",
    "                continue\n",
    "\n",
    "            rel_tokens = triple.relationTokens\n",
    "            simplified_rel_tokens = []\n",
    "            for rel_token in rel_tokens:\n",
    "                token_obj = ann.sentence[rel_token.sentenceIndex].token[rel_token.tokenIndex]\n",
    "                if token_obj.pos != 'RB':\n",
    "                    if token_obj.pos in inflex:\n",
    "                        simplified_rel_tokens.append(token_obj.lemma)\n",
    "                    else:\n",
    "                        simplified_rel_tokens.append(token_obj.word)\n",
    "            if len(simplified_rel_tokens) == 0:\n",
    "                #print(\"no rel\")\n",
    "                continue\n",
    "            else:\n",
    "                rel = \" \".join(simplified_rel_tokens)\n",
    "\n",
    "            for k, v in ioc_dic.items():\n",
    "                if k in sub:\n",
    "                    sub = sub.replace(k, v)\n",
    "                if k in obj:\n",
    "                    obj = obj.replace(k, v)\n",
    "                if k in rel:\n",
    "                    rel = rel.replace(k, v)\n",
    "\n",
    "            tri = Triple(sub, rel, obj, sub_ent=sub_entity, obj_ent=obj_entity, sub_ent_conf=sub_ner_conf,\n",
    "                         obj_ent_conf=obj_ner_conf, conf=triple.confidence)\n",
    "            tri.set_sent_ind(sent_ind)\n",
    "            tri.set_relative_sent_ind(sent_ind / len(ann.sentence))\n",
    "            tri.set_article_ref(doc_id)\n",
    "            tri.set_raw_triple(triple)\n",
    "            extracted.append(tri)\n",
    "            sub_counter.update([sub])\n",
    "            obj_counter.update([obj])\n",
    "            relation_counter.update([rel])\n",
    "    return extracted, (sub_counter, obj_counter), relation_counter\n",
    "\n",
    "\n",
    "def change_ioc(text):\n",
    "    count = 0\n",
    "    dic = dict()\n",
    "    textobj = IOCParser(text);\n",
    "    results = textobj.parse();\n",
    "    for res in results:\n",
    "        sig = res.kind + str(count)\n",
    "        dic[sig] = res.value\n",
    "        count += 1\n",
    "        text = text.replace(res.value, sig)\n",
    "    return text, dic\n",
    "\n",
    "tactic_dic = {\n",
    "    0: 'Initial Access', \n",
    "    1: 'Execution', \n",
    "    2: 'Defense Evasion', \n",
    "    3: 'Command and Control', \n",
    "    4: 'Privilege Escalation', \n",
    "    5: 'Persistence', \n",
    "    6: 'Lateral Movement', \n",
    "    7: 'DataLeak', \n",
    "    8: 'Exfiltration', \n",
    "    9: 'Impact'\n",
    "}\n",
    "\n",
    "\n",
    "valid_ner = [\n",
    "         'Known_Malware',\n",
    "         'PRO',\n",
    "         'PERSON',\n",
    "         'filename',\n",
    "         'uri',\n",
    "         'md5',\n",
    "         'ACTOR',\n",
    "         'Known_Actor',\n",
    "         'sha256',\n",
    "         'URL',\n",
    "         'sha1',\n",
    "         'CVE',\n",
    "]\n",
    "\n",
    "def dedupe(article_extracts):\n",
    "    seen_sent = dict()\n",
    "    unique = []\n",
    "    for triple in article_extracts:\n",
    "        sent_ind = triple.sent_ind\n",
    "        if sent_ind not in seen_sent:\n",
    "            seen_sent[sent_ind] = list()\n",
    "        \n",
    "        dup = False\n",
    "        for seen_triple in seen_sent[sent_ind]:\n",
    "            if seen_triple == triple:\n",
    "                if len(seen_triple.sub) >= len(triple.sub) and len(seen_triple.obj) >= len(triple.obj):\n",
    "                    seen_sent[sent_ind].remove(seen_triple)\n",
    "                    dup = False\n",
    "                else:\n",
    "                    #seen_triple.set_count(seen_triple.count+1)\n",
    "                    dup = True\n",
    "                break\n",
    "                    \n",
    "        if not dup:\n",
    "            seen_sent[sent_ind].append(triple)\n",
    "    \n",
    "    for k, v in seen_sent.items():\n",
    "        unique += v\n",
    "        \n",
    "    return unique\n",
    "  \n",
    "def fusion(trip):\n",
    "    det_ner = ['filename', 'sha256', 'CVE', 'uri', 'md5', 'sha1']\n",
    "    if trip.sub_ent not in det_ner:\n",
    "        for single in malware:\n",
    "            #if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub = single\n",
    "                trip.sub_ent = 'Known_Malware'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub = single\n",
    "                trip.sub_ent = 'Known_Malware'\n",
    "                trip.sub_ent_conf = 1\n",
    "                \n",
    "        for single in software:\n",
    "           # if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub_ent = 'PRO'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub_ent = 'PRO'\n",
    "                trip.sub_ent_conf = 1\n",
    "                \n",
    "        for single in actor:\n",
    "            #if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub_ent = 'Known_Actor'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub_ent = 'Known_Actor'\n",
    "                trip.sub_ent_conf = 1\n",
    "    \n",
    "    if trip.obj_ent not in det_ner:\n",
    "        for single in malware:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj = single\n",
    "                trip.obj_ent = 'Known_Malware'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj = single\n",
    "                trip.obj_ent = 'Known_Malware'\n",
    "                trip.obj_ent_conf = 1\n",
    "                \n",
    "        for single in software:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj_ent = 'PRO'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj_ent = 'PRO'\n",
    "                trip.obj_ent_conf = 1\n",
    "                \n",
    "        for single in actor:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj_ent = 'Known_Actor'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj_ent = 'Known_Actor'\n",
    "                trip.obj_ent_conf = 1\n",
    "\n",
    "def filter_ner(triples):\n",
    "    filtered = []\n",
    "    for triple in triples:\n",
    "        if triple.sub_ent != '' and triple.sub_ent in valid_ner and triple.obj_ent != '' and triple.obj_ent in valid_ner:\n",
    "            filtered.append(triple)\n",
    "            \n",
    "    return filtered\n",
    "\n",
    "def filter_conf(triples, behav_thresh=0.5):\n",
    "    filtered = []\n",
    "    for triple in triples:        \n",
    "        if triple.confidence < 0.8:\n",
    "            continue\n",
    "            \n",
    "        if triple.behave_conf < behav_thresh:\n",
    "            continue\n",
    "        \n",
    "        filtered.append(triple)\n",
    "    return filtered\n",
    "\n",
    "#%%\n",
    "import stanza\n",
    "corenlp_dir = '/home/user/anaconda3/data/corenlp'\n",
    "#stanza.install_corenlp(dir='/home/user/anaconda3/data/corenlp')\n",
    "from stanza.server import CoreNLPClient\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] ='/home/user/anaconda3/data/corenlp'\n",
    "word_set = set(words.words())\n",
    "sys.modules['CoreNLP_pb2'] = CoreNLP_pb2\n",
    "\n",
    "\n",
    "w2v = pickle.load(open('pkl/w2v_ref_blog_lemma.pkl', 'rb'))\n",
    "\n",
    "mitre_groups = pickle.load(open('pkl/mitre_groups.pkl', 'rb'))\n",
    "mitre_malwares = pickle.load(open('pkl/mitre_malwares.pkl', 'rb'))\n",
    "programm_language = pickle.load(open('pkl/programming_language.pkl', 'rb'))\n",
    "\n",
    "mitre_groups = set(list(map(str.lower, mitre_groups)))\n",
    "mitre_malwares = set(list(map(str.lower, mitre_malwares)))\n",
    "\n",
    "malware = pickle.load(open('pkl/malware.pkl', 'rb'))\n",
    "software = pickle.load(open('pkl/software.pkl', 'rb'))\n",
    "actor = pickle.load(open('pkl/actor.pkl', 'rb'))\n",
    "\n",
    "mal_suffix = ['rat', 'downloader', 'trojan', 'ransomware', 'malware', 'virus', 'worm', 'backdoor']\n",
    "for malname in list(malware):\n",
    "    if ' ' in malname:\n",
    "        for _ in mal_suffix:\n",
    "            if _ in malname.split(' '):\n",
    "                malware.remove(malname)\n",
    "                malware.add(''.join(filter(lambda x: x not in mal_suffix, malname.split(' '))))\n",
    "\n",
    "for malname in list(malware):\n",
    "    if malname in word_set:\n",
    "        malware.remove(malname)\n",
    "\n",
    "for softname in list(software):\n",
    "    if softname in word_set:\n",
    "        software.remove(softname)\n",
    "\n",
    "for actname in list(actor):\n",
    "    if actname in word_set:\n",
    "        actor.remove(actname)\n",
    "\n",
    "\n",
    "class Triple():\n",
    "    def __init__(self, sub, rel, obj, sub_ent='', rel_ent='', obj_ent='', sub_ent_conf=0, obj_ent_conf=0, conf=0):\n",
    "        self.sub = sub.lower()\n",
    "        self.rel = rel.lower()\n",
    "        self.obj = obj.lower()\n",
    "        self.sub_ent = sub_ent\n",
    "        self.rel_ent = rel_ent\n",
    "        self.obj_ent = obj_ent\n",
    "        self.sub_ent_conf = sub_ent_conf\n",
    "        self.obj_ent_conf = obj_ent_conf\n",
    "        self.confidence = conf\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.simple_sub + self.simple_rel + self.simple_obj)\n",
    "\n",
    "    def set_raw_article_doc(self, doc):\n",
    "        self.article_doc = doc\n",
    "\n",
    "    def set_raw_sent(self, sent):\n",
    "        self.raw_sent = sent\n",
    "\n",
    "    def set_raw_triple(self, triple_obj):\n",
    "        self.triple_obj = triple_obj\n",
    "\n",
    "    def set_sent_ind(self, sent_ind):\n",
    "        self.sent_ind = sent_ind\n",
    "\n",
    "    def set_article_ref(self, article):\n",
    "        self.article_id = article\n",
    "\n",
    "    def set_tactic(self, tactic):\n",
    "        self.tactic = tactic\n",
    "\n",
    "    def set_tactic_conf(self, tactic_conf):\n",
    "        self.tactic_conf = tactic_conf\n",
    "\n",
    "    def set_behave_conf(self, behave_conf):\n",
    "        self.behave_conf = behave_conf\n",
    "\n",
    "    def set_relative_sent_ind(self, relative_sent_order):\n",
    "        self.relative_sent_order = relative_sent_order\n",
    "\n",
    "    def set_count(self, count):\n",
    "        self.count = count\n",
    "\n",
    "    def add_cluster(self, cluster):\n",
    "        self.from_cluster = set()\n",
    "        self.from_cluster.add(cluster)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        sub_eq = False\n",
    "        if self.sub in other.sub or other.sub in self.sub:\n",
    "            sub_eq = True\n",
    "        elif fuzz.ratio(self.sub, other.sub) > 90:\n",
    "            sub_eq = True\n",
    "        else:\n",
    "            self_sub_tokens = self.sub.split(' ')\n",
    "            other_sub_tokens = other.sub.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_sub_tokens)) and all(map(lambda x: x in w2v.wv, other_sub_tokens)):\n",
    "                self_sub_vec = np.mean([w2v.wv[x] for x in self_sub_tokens], axis=0)\n",
    "                other_sub_vec = np.mean([w2v.wv[x] for x in other_sub_tokens], axis=0)\n",
    "                sub_eq = spatial.distance.cosine(self_sub_vec, other_sub_vec) < 0.3\n",
    "\n",
    "        rel_eq = False\n",
    "        if self.rel in other.rel or other.rel in self.rel:\n",
    "            rel_eq = True\n",
    "        elif self.rel == other.rel:\n",
    "            rel_eq = True\n",
    "        else:\n",
    "            self_rel_tokens = self.rel.split(' ')\n",
    "            other_rel_tokens = other.rel.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_rel_tokens)) and all(map(lambda x: x in w2v.wv, other_rel_tokens)):\n",
    "                self_rel_vec = np.mean([w2v.wv[x] for x in self_rel_tokens], axis=0)\n",
    "                other_rel_vec = np.mean([w2v.wv[x] for x in other_rel_tokens], axis=0)\n",
    "                rel_eq = spatial.distance.cosine(self_rel_vec, other_rel_vec) < 0.3\n",
    "\n",
    "        obj_eq = False\n",
    "        if self.obj in other.obj or other.obj in self.obj:\n",
    "            obj_eq = True\n",
    "        elif fuzz.ratio(self.obj, other.obj) > 90:\n",
    "            obj_eq = True\n",
    "        else:\n",
    "            self_obj_tokens = self.obj.split(' ')\n",
    "            other_obj_tokens = other.obj.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_obj_tokens)) and all(map(lambda x: x in w2v.wv, other_obj_tokens)):\n",
    "                self_obj_vec = np.mean([w2v.wv[x] for x in self_obj_tokens], axis=0)\n",
    "                other_obj_vec = np.mean([w2v.wv[x] for x in other_obj_tokens], axis=0)\n",
    "                obj_eq = spatial.distance.cosine(self_obj_vec, other_obj_vec) < 0.3\n",
    "\n",
    "        return sub_eq and rel_eq and obj_eq and self.sub_ent == other.sub_ent and self.obj_ent == other.obj_ent and self.rel_ent == other.rel_ent\n",
    "\n",
    "    def print(self):\n",
    "        #print(f'{self.sub}({self.sub_ent}) >> {self.rel} >> {self.obj}({self.obj_ent})')\n",
    "        use_less='1'\n",
    "\n",
    "\n",
    "def bert_ner(ann):\n",
    "    tokenized = [[x.value for x in sent.token] for sent in ann.sentence]\n",
    "    predictions, raw_outputs = model.predict(tokenized, split_on_space=False)\n",
    "    return predictions, raw_outputs\n",
    "\n",
    "\n",
    "def simple_match(candidate, ioc_dic):\n",
    "    candidate = candidate.lower().strip()\n",
    "    if (candidate.startswith(\"cve\")):\n",
    "        return \"CVE\"\n",
    "    for _ in mitre_groups:\n",
    "        if candidate.find(_) != -1:\n",
    "            return \"Known_Actor\"\n",
    "    for _ in mitre_malwares:\n",
    "        if candidate.find(_) != -1:\n",
    "            return \"Known_Malware\"\n",
    "    for _ in programm_language:\n",
    "        if _ == candidate:\n",
    "            return \"Programming_Language\"\n",
    "    for _ in ioc_dic:\n",
    "        if _ in candidate:\n",
    "            textobj = IOCParser(ioc_dic[_]);\n",
    "            results = textobj.parse();\n",
    "            if len(results) > 0:\n",
    "                return results[0].kind\n",
    "    return ''\n",
    "\n",
    "\n",
    "def process_triple_tokens(tokens, ann, predictions, raws, ioc_dic):\n",
    "    redundant = ['PRP$', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'DT']\n",
    "    inflex = ['VBD', 'VBG', 'VBP', 'VBZ', 'NNS', 'NNPS']\n",
    "    sub_tokens = tokens\n",
    "    simplified_sub_tokens = []\n",
    "    sub_entity = ''\n",
    "    sub_ner_conf = 0\n",
    "    for sub_token in sub_tokens:\n",
    "        token_obj = ann.sentence[sub_token.sentenceIndex].token[sub_token.tokenIndex]\n",
    "        if token_obj.pos.startswith('NN'):\n",
    "            ner_tag = 'O'\n",
    "            try:\n",
    "                ner_tag = list(predictions[sub_token.sentenceIndex][sub_token.tokenIndex].values())[0]\n",
    "                sub_ner_conf = max(\n",
    "                    scipy.special.softmax(list(raws[sub_token.sentenceIndex][sub_token.tokenIndex].values())[0][0]))\n",
    "            except:\n",
    "                pass\n",
    "            if ner_tag.startswith('B'):\n",
    "                sub_entity = ner_tag.split('-')[1]\n",
    "            if token_obj.pos in inflex:\n",
    "                simplified_sub_tokens.append(token_obj.lemma)\n",
    "            else:\n",
    "                simplified_sub_tokens.append(token_obj.word)\n",
    "        elif token_obj.pos not in redundant:\n",
    "            simplified_sub_tokens.append(token_obj.word)\n",
    "    if len(simplified_sub_tokens) == 0:\n",
    "        return None, None, None\n",
    "    else:\n",
    "        sub = \" \".join(simplified_sub_tokens)\n",
    "        simple_ent = simple_match(sub, ioc_dic)\n",
    "        if simple_ent != '':\n",
    "            sub_entity = simple_ent\n",
    "            sub_ner_conf = 1\n",
    "    return sub, sub_entity, sub_ner_conf\n",
    "\n",
    "\n",
    "def extract_triples_with_ner(ann, predictions, raws, ioc_dic, doc_id=None):\n",
    "    inflex = ['VBD', 'VBG', 'VBP', 'VBZ', 'NNS', 'NNPS']\n",
    "    extracted = list()\n",
    "    sub_counter = Counter()\n",
    "    obj_counter = Counter()\n",
    "    relation_counter = Counter()\n",
    "    for sent_ind, sent in enumerate(ann.sentence):\n",
    "        triples = sent.openieTriple\n",
    "        for triple in triples:\n",
    "            #print(triple)\n",
    "            sub_tokens = triple.subjectTokens\n",
    "            sub, sub_entity, sub_ner_conf = process_triple_tokens(sub_tokens, ann, predictions, raws, ioc_dic)\n",
    "            if not sub:\n",
    "                continue\n",
    "\n",
    "            obj_tokens = triple.objectTokens\n",
    "            obj, obj_entity, obj_ner_conf = process_triple_tokens(obj_tokens, ann, predictions, raws, ioc_dic)\n",
    "            if not obj:\n",
    "                continue\n",
    "\n",
    "            rel_tokens = triple.relationTokens\n",
    "            simplified_rel_tokens = []\n",
    "            for rel_token in rel_tokens:\n",
    "                token_obj = ann.sentence[rel_token.sentenceIndex].token[rel_token.tokenIndex]\n",
    "                if token_obj.pos != 'RB':\n",
    "                    if token_obj.pos in inflex:\n",
    "                        simplified_rel_tokens.append(token_obj.lemma)\n",
    "                    else:\n",
    "                        simplified_rel_tokens.append(token_obj.word)\n",
    "            if len(simplified_rel_tokens) == 0:\n",
    "                #print(\"no rel\")\n",
    "                continue\n",
    "            else:\n",
    "                rel = \" \".join(simplified_rel_tokens)\n",
    "\n",
    "            for k, v in ioc_dic.items():\n",
    "                if k in sub:\n",
    "                    sub = sub.replace(k, v)\n",
    "                if k in obj:\n",
    "                    obj = obj.replace(k, v)\n",
    "                if k in rel:\n",
    "                    rel = rel.replace(k, v)\n",
    "\n",
    "            tri = Triple(sub, rel, obj, sub_ent=sub_entity, obj_ent=obj_entity, sub_ent_conf=sub_ner_conf,\n",
    "                         obj_ent_conf=obj_ner_conf, conf=triple.confidence)\n",
    "            tri.set_sent_ind(sent_ind)\n",
    "            tri.set_relative_sent_ind(sent_ind / len(ann.sentence))\n",
    "            tri.set_article_ref(doc_id)\n",
    "            tri.set_raw_triple(triple)\n",
    "            extracted.append(tri)\n",
    "            sub_counter.update([sub])\n",
    "            obj_counter.update([obj])\n",
    "            relation_counter.update([rel])\n",
    "    return extracted, (sub_counter, obj_counter), relation_counter\n",
    "\n",
    "\n",
    "def change_ioc(text):\n",
    "    count = 0\n",
    "    dic = dict()\n",
    "    textobj = IOCParser(text);\n",
    "    results = textobj.parse();\n",
    "    for res in results:\n",
    "        sig = res.kind + str(count)\n",
    "        dic[sig] = res.value\n",
    "        count += 1\n",
    "        text = text.replace(res.value, sig)\n",
    "    return text, dic\n",
    "\n",
    "tactic_dic = {\n",
    "    0: 'Initial Access', \n",
    "    1: 'Execution', \n",
    "    2: 'Defense Evasion', \n",
    "    3: 'Command and Control', \n",
    "    4: 'Privilege Escalation', \n",
    "    5: 'Persistence', \n",
    "    6: 'Lateral Movement', \n",
    "    7: 'DataLeak', \n",
    "    8: 'Exfiltration', \n",
    "    9: 'Impact'\n",
    "}\n",
    "\n",
    "\n",
    "valid_ner = [\n",
    "         'Known_Malware',\n",
    "         'PRO',\n",
    "         'PERSON',\n",
    "         'filename',\n",
    "         'uri',\n",
    "         'md5',\n",
    "         'ACTOR',\n",
    "         'Known_Actor',\n",
    "         'sha256',\n",
    "         'URL',\n",
    "         'sha1',\n",
    "         'CVE',\n",
    "]\n",
    "\n",
    "def dedupe(article_extracts):\n",
    "    seen_sent = dict()\n",
    "    unique = []\n",
    "    for triple in article_extracts:\n",
    "        sent_ind = triple.sent_ind\n",
    "        if sent_ind not in seen_sent:\n",
    "            seen_sent[sent_ind] = list()\n",
    "        \n",
    "        dup = False\n",
    "        for seen_triple in seen_sent[sent_ind]:\n",
    "            if seen_triple == triple:\n",
    "                if len(seen_triple.sub) >= len(triple.sub) and len(seen_triple.obj) >= len(triple.obj):\n",
    "                    seen_sent[sent_ind].remove(seen_triple)\n",
    "                    dup = False\n",
    "                else:\n",
    "                    #seen_triple.set_count(seen_triple.count+1)\n",
    "                    dup = True\n",
    "                break\n",
    "                    \n",
    "        if not dup:\n",
    "            seen_sent[sent_ind].append(triple)\n",
    "    \n",
    "    for k, v in seen_sent.items():\n",
    "        unique += v\n",
    "        \n",
    "    return unique\n",
    "  \n",
    "def fusion(trip):\n",
    "    det_ner = ['filename', 'sha256', 'CVE', 'uri', 'md5', 'sha1']\n",
    "    if trip.sub_ent not in det_ner:\n",
    "        for single in malware:\n",
    "            #if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub = single\n",
    "                trip.sub_ent = 'Known_Malware'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub = single\n",
    "                trip.sub_ent = 'Known_Malware'\n",
    "                trip.sub_ent_conf = 1\n",
    "                \n",
    "        for single in software:\n",
    "           # if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub_ent = 'PRO'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub_ent = 'PRO'\n",
    "                trip.sub_ent_conf = 1\n",
    "                \n",
    "        for single in actor:\n",
    "            #if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub_ent = 'Known_Actor'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub_ent = 'Known_Actor'\n",
    "                trip.sub_ent_conf = 1\n",
    "    \n",
    "    if trip.obj_ent not in det_ner:\n",
    "        for single in malware:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj = single\n",
    "                trip.obj_ent = 'Known_Malware'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj = single\n",
    "                trip.obj_ent = 'Known_Malware'\n",
    "                trip.obj_ent_conf = 1\n",
    "                \n",
    "        for single in software:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj_ent = 'PRO'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj_ent = 'PRO'\n",
    "                trip.obj_ent_conf = 1\n",
    "                \n",
    "        for single in actor:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj_ent = 'Known_Actor'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj_ent = 'Known_Actor'\n",
    "                trip.obj_ent_conf = 1\n",
    "\n",
    "def filter_ner(triples):\n",
    "    filtered = []\n",
    "    for triple in triples:\n",
    "        if triple.sub_ent != '' and triple.sub_ent in valid_ner and triple.obj_ent != '' and triple.obj_ent in valid_ner:\n",
    "            filtered.append(triple)\n",
    "            \n",
    "    return filtered\n",
    "\n",
    "def filter_conf(triples, behav_thresh=0.5):\n",
    "    filtered = []\n",
    "    for triple in triples:        \n",
    "        if triple.confidence < 0.8:\n",
    "            continue\n",
    "            \n",
    "        if triple.behave_conf < behav_thresh:\n",
    "            continue\n",
    "        \n",
    "        filtered.append(triple)\n",
    "    return filtered\n",
    "\n",
    "import numpy as np\n",
    "import glob,pickle\n",
    "import sys\n",
    "from simpletransformers.classification import ClassificationModel,    ClassificationArgs,    MultiLabelClassificationModel # type: ignore\n",
    "\n",
    "from simpletransformers.ner import NERModel\n",
    "model = NERModel(\"roberta\", \"nermodel\",use_cuda=True)\n",
    "be_model=MultiLabelClassificationModel(\"roberta\", 'behavior model',use_cuda=True)\n",
    "ta_model=MultiLabelClassificationModel(\"roberta\", 'tactic model',use_cuda=True)\n",
    "\n",
    "def find_zeros(lst):\n",
    "    zeros_count = 0\n",
    "    zeros_indices = []\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] == 0:\n",
    "            zeros_count += 1\n",
    "        else:\n",
    "            if zeros_count > 0:\n",
    "                zeros_indices.append(i - zeros_count)\n",
    "            zeros_count = 0\n",
    "    if zeros_count > 0:\n",
    "        zeros_indices.append(len(lst) - zeros_count)\n",
    "    return zeros_indices\n",
    "\n",
    "def nlp_process(dict_text_ann):\n",
    "  anns = list(dict_text_ann.values())\n",
    "  keys = list(dict_text_ann.keys())\n",
    "  texts=[]\n",
    "  tokenized_s=[]\n",
    "  anns_noerror=[]\n",
    "  for ann in anns:\n",
    "    if ann != 'error':\n",
    "      anns_noerror.append(ann)\n",
    "      tokenized_s.append([[x.value for x in sent.token] for sent in ann.sentence])\n",
    "      texts.append(ann.text)\n",
    "\n",
    "  tokenized_s=[x[0] for x in tokenized_s]\n",
    "  prediction, raw_output = model.predict(tokenized_s, split_on_space=False)\n",
    "\n",
    "  extracted_listS=[]\n",
    "  for i in range(len(anns_noerror)):\n",
    "    ann=anns_noerror[i]\n",
    "    ner=prediction[i]\n",
    "    raw_out=raw_output[i]\n",
    "    notext, ioc_dic = change_ioc(texts[i])\n",
    "    extracted_list, entity_count, rel_count = extract_triples_with_ner(ann, ner, raw_out, ioc_dic, doc_id = 1)\n",
    "    extracted_listS.append(extracted_list)\n",
    "\n",
    "  text_list=texts\n",
    "\n",
    "  be_predictions, be_raw_outputs = be_model.predict(text_list)\n",
    "  ta_predictions, ta_raw_outputs = ta_model.predict(text_list)\n",
    "\n",
    "  for i in range(len(extracted_listS)):\n",
    "      if len(be_predictions[i])>0:\n",
    "          for triple in extracted_listS[i]:\n",
    "              triple.raw_sent=text_list[i]\n",
    "              triple.behave_conf=be_raw_outputs[i][1]\n",
    "              triple.tactic=find_zeros(ta_predictions[i])\n",
    "              triple.sent_ind=i\n",
    "              triple.tactic_conf=ta_predictions[i]\n",
    "\n",
    "  for i in range(len(extracted_listS)):\n",
    "    extracted_listS[i]=dedupe(extracted_listS[i])\n",
    "  for i in range(len(extracted_listS)):\n",
    "    for j in range(len(extracted_listS[i])):\n",
    "      fusion(extracted_listS[i][j])\n",
    "  return extracted_listS\n",
    "#%%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Files\n",
    "import os\n",
    "import glob\n",
    "files = glob.glob('/home/user/Dropbox (XXXXX)/nosync/noerrorfinal/*.pkl')\n",
    "print(len(files))\n",
    "all_triples=[]\n",
    "from tqdm import tqdm\n",
    "for file in tqdm(files):\n",
    "    pkl=pickle.load(open(file,'rb'))\n",
    "    flat_pkl=[item for i in pkl for item in i]\n",
    "    for i in flat_pkl:\n",
    "        i.article_id=file.split('/')[-1].split('.')[0]\n",
    "    all_triples.extend(flat_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Graph From ALL Triples, With Name 'G_full'\n",
    "import networkx as nx\n",
    "triples = all_triples\n",
    "G_full = nx.DiGraph()\n",
    "\n",
    "for triple in triples:\n",
    "    sub = triple.sub\n",
    "    obj = triple.obj\n",
    "    rel = triple.rel\n",
    "    if sub not in G_full:\n",
    "        G_full.add_node(sub, entity=triple.sub_ent, entity_conf=triple.sub_ent_conf)\n",
    "    if obj not in G_full:\n",
    "        G_full.add_node(obj, entity=triple.obj_ent, entity_conf=triple.obj_ent_conf)\n",
    "\n",
    "    G_full.add_edge(sub, obj, relation=rel, confidence=triple.confidence, sent_ind=triple.sent_ind,\n",
    "               article_id=triple.article_id, raw_sent=triple.raw_sent, behave_conf=triple.behave_conf,\n",
    "               tactic=triple.tactic, tactic_conf=triple.tactic_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Only Use CTI Articles To Build The Graph, With Name 'G_cti'\n",
    "data85k_fulltext=pd.read_csv('/home/user/Dropbox (XXXXX)/code/df_articles.csv')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "thresholds=[0.15]\n",
    "for threshold in thresholds:\n",
    "    data85k_text_value_article = data85k_fulltext[data85k_fulltext['label_value'] > threshold]\n",
    "    data85k_text_value_article_sentence = set(data85k_text_value_article['sentence'].tolist())\n",
    "    filter_triple = [triple for triple in tqdm(triples) if triple.raw_sent in data85k_text_value_article_sentence]\n",
    "    G_Value = nx.DiGraph()\n",
    "    for triple in tqdm(filter_triple):\n",
    "        sub = triple.sub\n",
    "        obj = triple.obj\n",
    "        rel = triple.rel\n",
    "        if sub not in G_Value:\n",
    "            G_Value.add_node(sub, entity=triple.sub_ent, entity_conf=triple.sub_ent_conf)\n",
    "        if obj not in G_Value:\n",
    "            G_Value.add_node(obj, entity=triple.obj_ent, entity_conf=triple.obj_ent_conf)\n",
    "        G_Value.add_edge(sub, obj, relation=rel, confidence=triple.confidence, sent_ind=triple.sent_ind,\n",
    "                        article_id=triple.article_id, raw_sent=triple.raw_sent, behave_conf=triple.behave_conf,\n",
    "                        tactic=triple.tactic, tactic_conf=triple.tactic_conf)\n",
    "    isolated_nodes = list(nx.isolates(G_Value))\n",
    "    G_Value.remove_nodes_from(isolated_nodes)\n",
    "    number_of_nodes = G_Value.number_of_nodes()\n",
    "    number_of_edges = G_Value.number_of_edges()\n",
    "    results_df = results_df.append({'Threshold': threshold, 'Number of Nodes': number_of_nodes, 'Number of Edges': number_of_edges}, ignore_index=True)\n",
    "print(results_df)\n",
    "\n",
    "original_number_of_nodes = G_full.number_of_nodes()\n",
    "original_number_of_edges = G_full.number_of_edges()\n",
    "\n",
    "results_df['Node Ratio (%)'] = round(results_df['Number of Nodes'] / original_number_of_nodes * 100, 2)\n",
    "results_df['Edge Ratio (%)'] = round(results_df['Number of Edges'] / original_number_of_edges * 100, 2)\n",
    "results_df['Threshold']=results_df['Threshold'].apply(lambda x:round(x,2))\n",
    "results_df = results_df[['Threshold','Number of Nodes','Node Ratio (%)','Number of Edges','Edge Ratio (%)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: Plot The Degree Distribution Of Full Graph And CTI Graph\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "def count_behaviors(G, node):\n",
    "    edges = list(G.in_edges(node, data=True)) + list(G.out_edges(node, data=True))\n",
    "    \n",
    "    article_ids = [edge[2]['article_id'] for edge in edges]\n",
    "    \n",
    "    article_id_counts = Counter(article_ids)\n",
    "    \n",
    "    base_article_id = max(article_id_counts, key=article_id_counts.get)\n",
    "    base_behaviors = article_id_counts[base_article_id]\n",
    "\n",
    "    extend_edges = [edge for edge in edges if edge[2]['article_id'] != base_article_id]\n",
    "    extend_behaviors = len(extend_edges)\n",
    "\n",
    "    extend_article_ids = list(set([edge[2]['article_id'] for edge in extend_edges]))\n",
    "    extend_articles = len(extend_article_ids)\n",
    "    \n",
    "    return base_behaviors, extend_behaviors, extend_articles\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_node_behavior(G,entity_type,not_want_list):\n",
    "    filtered_nodes = [node for node, attr in G.nodes(data=True) if attr['entity'] == entity_type and node.lower() not in not_want_list]\n",
    "    results = []\n",
    "    total_base_behaviors = 0\n",
    "    total_extend_behaviors = 0\n",
    "    total_extend_articles = 0\n",
    "    total_degrees = 0\n",
    "\n",
    "    for node in filtered_nodes:\n",
    "        base_behaviors, extend_behaviors, extend_articles = count_behaviors(G, node)\n",
    "        total_base_behaviors += base_behaviors\n",
    "        total_extend_behaviors += extend_behaviors\n",
    "        total_extend_articles += extend_articles\n",
    "        total_degrees += G.degree(node)\n",
    "\n",
    "        results.append((node, base_behaviors + extend_behaviors, base_behaviors, extend_behaviors, extend_articles))\n",
    "\n",
    "    top10_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    average_base_behaviors = total_base_behaviors / len(filtered_nodes)\n",
    "    average_extend_behaviors = total_extend_behaviors / len(filtered_nodes)\n",
    "    average_extend_articles = total_extend_articles / len(filtered_nodes)\n",
    "    average_degrees = total_degrees / len(filtered_nodes)\n",
    "    \n",
    "    average_results = {\n",
    "        \"Average Base Behavior\": average_base_behaviors,\n",
    "        \"Average Extend Behavior\": average_extend_behaviors,\n",
    "        \"Average Extend from article number\": average_extend_articles,\n",
    "        \"Average Total Degree\": average_degrees,\n",
    "    }\n",
    "    \n",
    "    node_behavior_data = {\n",
    "        \"Node\": [],\n",
    "        \"Total Degree\": [],\n",
    "        \"Base Behavior\": [],\n",
    "        \"Extend Behavior\": [],\n",
    "        \"Extend from article number\": [],\n",
    "    }\n",
    "\n",
    "    for node, total_degree, base_behavior, extend_behavior, extend_from_article_number in top10_results:\n",
    "        node_behavior_data[\"Node\"].append(node)\n",
    "        node_behavior_data[\"Total Degree\"].append(total_degree)\n",
    "        node_behavior_data[\"Base Behavior\"].append(base_behavior)\n",
    "        node_behavior_data[\"Extend Behavior\"].append(extend_behavior)\n",
    "        node_behavior_data[\"Extend from article number\"].append(extend_from_article_number)\n",
    "    \n",
    "    node_behavior_df = pd.DataFrame(node_behavior_data)\n",
    "    \n",
    "    return average_results, node_behavior_df\n",
    "\n",
    "average_results, node_behavior_df = analyze_node_behavior(G=G_Value, entity_type='CVE',not_want_list=['cve','cve number'])\n",
    "\n",
    "for key, value in average_results.items():\n",
    "    print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "node_behavior_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
